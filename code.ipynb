{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown-1",
   "metadata": {},
   "source": [
    "# Building a Knowledge Graph with Small Language Models: A Comparative Approach\n",
    "\n",
    "This notebook demonstrates the process of building a Knowledge Graph from unstructured text using small, locally-run Language Models (LLMs). We will explore two primary approaches:\n",
    "\n",
    "1.  **The Standard LangChain Approach**: Using the built-in `LLMGraphTransformer` from LangChain. We will see the challenges and limitations, particularly the high failure rate in extracting structured graph data with smaller models like Mistral.\n",
    "2.  **The BAML-Enhanced Approach**: Leveraging Boundary's AI-programming language (BAML) to significantly improve the reliability and accuracy of graph extraction from a model like Llama 3.\n",
    "\n",
    "Finally, after successfully generating high-quality graph documents with BAML, we will ingest them into a Neo4j graph database and perform various analyses, including community detection and entity resolution, to uncover insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0fbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic command to load the autoreload extension.\n",
    "%load_ext autoreload\n",
    "# Magic command to automatically reload all modules before executing a cell.\n",
    "# This is useful for development when you are changing the source code of imported modules.\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "## Part 1: The Standard LangChain Approach with `LLMGraphTransformer`\n",
    "\n",
    "In this section, we'll start with the standard method for graph extraction provided by LangChain. We will use `LLMGraphTransformer`, which is designed to take a sequence of documents and convert them into graph documents. We'll use the `mistral` model, a popular small LLM, to see how well this approach works out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff7a098-imports-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from LangChain and other standard packages.\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_ollama import ChatOllama\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# Import specific data structures for handling graph data.\n",
    "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215685d9-mistral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name to be used.\n",
    "model = \"mistral\"\n",
    "\n",
    "# Initialize the ChatOllama instance.\n",
    "# This connects to a locally running Ollama service to use the specified model.\n",
    "# We set a very low temperature to make the model's output more deterministic and less random.\n",
    "llm = ChatOllama(model=model, temperature=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-output-intro",
   "metadata": {},
   "source": [
    "### Testing Structured Output with Pydantic\n",
    "\n",
    "Before diving into the complex task of graph extraction, let's test the LLM's ability to produce structured output using a simpler Pydantic model. LangChain's `.with_structured_output` method allows us to specify a schema (like a Pydantic class) that the LLM should conform to. This is a good way to gauge the model's instruction-following capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13eba915-joke-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model named 'Joke'.\n",
    "# This class specifies the desired structure for a joke, with a 'setup' and a 'punchline'.\n",
    "# The Field descriptions guide the LLM on what content to generate for each attribute.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f850847-joke-invoke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOllama(model='mistral', temperature=0.001), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Joke', 'description': '', 'parameters': {'type': 'object', 'properties': {'setup': {'description': 'The setup of the joke', 'type': 'string'}, 'punchline': {'description': 'The punchline to the joke', 'type': 'string'}}, 'required': ['setup', 'punchline']}}}], 'tool_choice': 'any'}, config={}, config_factories=[])\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.Joke'>])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bind the LLM to the structured output format defined by the Joke class.\n",
    "# This creates a new LangChain Runnable that will automatically prompt the LLM to return a JSON object\n",
    "# matching the Joke schema.\n",
    "llm.with_structured_output(Joke)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-loading-markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "We will use a dataset of news articles. To manage the LLM's context window and processing time, it's crucial to understand the size of our text chunks. We'll use `tiktoken` to count the number of tokens in each article, giving us a good proxy for the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2d2521-dataset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevron: Best Of Breed</td>\n",
       "      <td>2031-04-06T01:36:32.000000000+00:00</td>\n",
       "      <td>JHVEPhoto Like many companies in the O&G secto...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FirstEnergy (NYSE:FE) Posts Earnings Results</td>\n",
       "      <td>2030-04-29T06:55:28.000000000+00:00</td>\n",
       "      <td>FirstEnergy (NYSE:FE – Get Rating) posted its ...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dáil almost suspended after Sinn Féin TD put p...</td>\n",
       "      <td>2023-06-15T14:32:11.000000000+00:00</td>\n",
       "      <td>The Dáil was almost suspended on Thursday afte...</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Epic’s latest tool can animate hyperrealistic ...</td>\n",
       "      <td>2023-06-15T14:00:00.000000000+00:00</td>\n",
       "      <td>Today, Epic is releasing a new tool designed t...</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EU to Ban Huawei, ZTE from Internal Commission...</td>\n",
       "      <td>2023-06-15T13:50:00.000000000+00:00</td>\n",
       "      <td>The European Commission is planning to ban equ...</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                             Chevron: Best Of Breed   \n",
       "1       FirstEnergy (NYSE:FE) Posts Earnings Results   \n",
       "2  Dáil almost suspended after Sinn Féin TD put p...   \n",
       "3  Epic’s latest tool can animate hyperrealistic ...   \n",
       "4  EU to Ban Huawei, ZTE from Internal Commission...   \n",
       "\n",
       "                                  date  \\\n",
       "0  2031-04-06T01:36:32.000000000+00:00   \n",
       "1  2030-04-29T06:55:28.000000000+00:00   \n",
       "2  2023-06-15T14:32:11.000000000+00:00   \n",
       "3  2023-06-15T14:00:00.000000000+00:00   \n",
       "4  2023-06-15T13:50:00.000000000+00:00   \n",
       "\n",
       "                                                text  tokens  \n",
       "0  JHVEPhoto Like many companies in the O&G secto...      78  \n",
       "1  FirstEnergy (NYSE:FE – Get Rating) posted its ...     130  \n",
       "2  The Dáil was almost suspended on Thursday afte...     631  \n",
       "3  Today, Epic is releasing a new tool designed t...     528  \n",
       "4  The European Commission is planning to ban equ...     281  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas for data manipulation and tiktoken for token counting.\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# Function to calculate the number of tokens in a string for a given model.\n",
    "def num_tokens_from_string(string: str, model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    # Get the encoding for the specified model.\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    # Encode the string and return the length of the resulting token list.\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# Load the news articles dataset from a remote CSV file.\n",
    "news = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\"\n",
    ")\n",
    "# Calculate the token count for each article by combining its title and text.\n",
    "news[\"tokens\"] = [\n",
    "    num_tokens_from_string(f\"{row['title']} {row['text']}\")\n",
    "    for i, row in news.iterrows()\n",
    "]\n",
    "# Display the first few rows of the DataFrame to inspect the data and token counts.\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662d76de-80c8-4760-873e-d961faed0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a histogram of the token counts to visualize the distribution of article lengths.\n",
    "sns.histplot(news[\"tokens\"], kde=False)\n",
    "plt.title(\"Distribution of chunk sizes\")\n",
    "plt.xlabel(\"Token count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-extraction-markdown",
   "metadata": {},
   "source": [
    "### Extracting Graph Documents\n",
    "\n",
    "Now we'll configure the `LLMGraphTransformer`. This component takes an LLM and prompts it to extract nodes and relationships from text. We specify that we want a `description` property for both nodes and relationships to capture more context.\n",
    "\n",
    "To speed up the process, we'll run the extraction on multiple articles concurrently using a `ThreadPoolExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e51216-transformer-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLMGraphTransformer with our local LLM.\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm, # The language model to use for extraction.\n",
    "    # Define the properties to extract for each node. Here, we only want a 'description'.\n",
    "    node_properties=[\"description\"],\n",
    "    # Define the properties to extract for each relationship.\n",
    "    relationship_properties=[\"description\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5197cd6-process-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to process a single piece of text.\n",
    "def process_text(text: str) -> List[GraphDocument]:\n",
    "    # Create a LangChain Document object from the input text.\n",
    "    doc = Document(page_content=text)\n",
    "    # Use the transformer to convert the document into a list of GraphDocument objects.\n",
    "    return llm_transformer.convert_to_graph_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c61ff0-threadpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 20/20 [01:32<00:00,  4.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for concurrent execution and progress tracking.\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the maximum number of worker threads.\n",
    "MAX_WORKERS = 10\n",
    "# Set the number of articles to process for this experiment.\n",
    "NUM_ARTICLES = 20\n",
    "# Initialize an empty list to store the results.\n",
    "graph_documents_mistral = []\n",
    "\n",
    "# Use a ThreadPoolExecutor for parallel processing.\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all text processing tasks to the executor.\n",
    "    futures = [\n",
    "        executor.submit(process_text, f\"{row['title']} {row['text']}\")\n",
    "        for i, row in news.head(NUM_ARTICLES).iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Iterate through the completed futures as they finish and show a progress bar.\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "    ):\n",
    "        # Get the result from the completed future.\n",
    "        graph_document = future.result()\n",
    "        # Extend the main list with the newly processed graph documents.\n",
    "        graph_documents_mistral.extend(graph_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a7ba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphDocument(nodes=[], relationships=[], source=Document(metadata={}, page_content='XPeng Stock Rises. The Tesla Rival Rolled Out Self-Driving Tech. Chinese electric-vehicle maker\\nXPeng\\nsaid Thursday its assisted-driving technology has been launched in Beijing and three other cities. The\\nTesla\\nrival’s stock was rising in premarket trading.'))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first result to inspect the output.\n",
    "# Note that for many articles, the nodes and relationships lists are empty, indicating a failure.\n",
    "graph_documents_mistral[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failure-analysis-markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Extraction Failure Rate\n",
    "\n",
    "A key indicator of success is whether the LLM was able to extract any nodes and relationships. If a `GraphDocument` object has no nodes, it means the LLM failed to parse the text and return the structured data in the expected format. Let's calculate the percentage of these failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d074ce-count-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for failed extractions (empty graph documents).\n",
    "empty_count = 0\n",
    "\n",
    "# Iterate through all the processed documents.\n",
    "for doc in graph_documents_mistral:\n",
    "    # Check if the 'nodes' list is empty.\n",
    "    if not doc.nodes:\n",
    "        # Increment the counter if no nodes were extracted.\n",
    "        empty_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4a09bd-print-failure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage missing: 75.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the percentage of documents for which the LLM failed to extract a graph.\n",
    "print(f\"Percentage missing: {empty_count/len(graph_documents_mistral)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-conclusion",
   "metadata": {},
   "source": [
    "As we can see, the standard `LLMGraphTransformer` with a small model like `mistral` has a **75% failure rate**. This is unacceptably high for building a reliable knowledge graph. The small LLM struggles to consistently adhere to the complex JSON format required by the transformer's internal prompt.\n",
    "\n",
    "This highlights the need for a more robust method to guide the LLM's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "## Part 2: The BAML-Enhanced Approach\n",
    "\n",
    "To address the high failure rate, we now turn to BAML (Boundary AI Markup Language). BAML is a configuration language that helps bridge the gap between natural language instructions and structured, typed outputs from LLMs. It allows us to define functions with clear input/output types, use Jinja for templating prompts, and set up robust parsing and retries, making it ideal for reliable structured data extraction.\n",
    "\n",
    "In this part, we will use BAML with the `llama3` model to perform the same graph extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa68d8f-baml-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BAML client, which is the interface to our BAML-defined functions.\n",
    "# This client is auto-generated from the `.baml` files in the `baml_src` directory.\n",
    "import baml_client as client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ff7a098-imports-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for the BAML approach.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import the custom system prompt defined for our graph extraction task.\n",
    "from prompts.graphragprompts import system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "215685d9-llama3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name. For this improved approach, we'll use Llama 3.\n",
    "model = \"llama3\"\n",
    "\n",
    "# Initialize the ChatOllama instance for Llama 3.\n",
    "llm = ChatOllama(model=model, temperature=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baml-chain-markdown",
   "metadata": {},
   "source": [
    "### Defining the BAML-Powered LangChain Runnable\n",
    "\n",
    "Here, we define a set of functions that will be chained together to create our final graph extraction pipeline. BAML functions are decorated with `@chain` to be seamlessly integrated into a LangChain Runnable sequence.\n",
    "\n",
    "- **Formatting Functions (`_format_nodes`, `_format_relationships`)**: These are utility functions to standardize the output from the LLM, ensuring node IDs and relationship types are consistently capitalized and formatted.\n",
    "- **`get_graph`**: This is an async BAML function (`b.ExtractGraph`) that takes the raw text and calls the LLM to extract nodes and relationships based on the schemas defined in our `.baml` files.\n",
    "- **`get_entities`**: This function is used later for entity resolution, calling another BAML function (`b.ExtractDeDupe`) to merge similar entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3677c3cf-baml-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components for building the chain.\n",
    "from typing import Any\n",
    "from baml_client.async_client import b\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_experimental.graph_transformers.llm import create_simple_model\n",
    "\n",
    "# Helper function to format the extracted nodes consistently.\n",
    "# It capitalizes the ID and type for standardization.\n",
    "def _format_nodes(nodes: List[Node]) -> List[Node]:\n",
    "    return [\n",
    "        Node(\n",
    "            id=el.id.title() if isinstance(el.id, str) else el.id,\n",
    "            type=el.type.capitalize() if el.type else None,\n",
    "            properties=(\n",
    "                el.properties.dict()\n",
    "                if hasattr(el.properties, \"dict\")\n",
    "                else el.properties\n",
    "            ),\n",
    "        )\n",
    "        for el in nodes\n",
    "    ]\n",
    "\n",
    "# Helper function to map the BAML relationship object to the base LangChain Relationship object.\n",
    "def map_to_base_relationship(rel: Any) -> Relationship:\n",
    "    \"\"\"Map the BAML Relationship to the base LangChain Relationship.\"\"\"\n",
    "    source = Node(id=rel.source_node_id, type=rel.source_node_type)\n",
    "    target = Node(id=rel.target_node_id, type=rel.target_node_type)\n",
    "    properties = {}\n",
    "    if hasattr(rel, \"properties\") and rel.properties:\n",
    "        properties = rel.properties.model_dump()\n",
    "    return Relationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )\n",
    "\n",
    "# Helper function to format a list of relationships.\n",
    "# It standardizes the source/target nodes and the relationship type (e.g., replacing spaces with underscores).\n",
    "def _format_relationships(rels) -> List[Relationship]:\n",
    "    relationships = [\n",
    "        map_to_base_relationship(rel)\n",
    "        for rel in rels\n",
    "        if rel.type and rel.source_node_id and rel.target_node_id\n",
    "    ]\n",
    "    return [\n",
    "        Relationship(\n",
    "            source=_format_nodes([el.source])[0],\n",
    "            target=_format_nodes([el.target])[0],\n",
    "            type=el.type.replace(\" \", \"_\").upper(),\n",
    "            properties=(\n",
    "                el.properties.dict()\n",
    "                if hasattr(el.properties, \"dict\")\n",
    "                else el.properties\n",
    "            ),\n",
    "        )\n",
    "        for el in relationships\n",
    "    ]\n",
    "\n",
    "# Decorate the function with @chain to make it a LangChain Runnable.\n",
    "# This function calls the BAML `ExtractGraph` function asynchronously.\n",
    "@chain\n",
    "async def get_graph(message):\n",
    "    graph = await b.ExtractGraph(graph=message.content)\n",
    "    return graph\n",
    "\n",
    "# Define another BAML-powered chain for deduplicating entities.\n",
    "# This will be used later in the notebook for entity resolution.\n",
    "@chain\n",
    "def get_entities(message):\n",
    "    entities = b.ExtractDeDupe(graph=message.content)\n",
    "    return entities.merged_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baml-prompt-markdown",
   "metadata": {},
   "source": [
    "### Assembling the LangChain Pipeline\n",
    "\n",
    "With our components defined, we now create the prompt template and chain them together into a single pipeline. The flow will be:\n",
    "\n",
    "1.  **Prompt**: Format the input text using our custom prompt template.\n",
    "2.  **LLM**: Send the formatted prompt to the `llama3` model.\n",
    "3.  **BAML Parser (`get_graph`)**: Take the raw LLM output and use BAML's robust parsing and type-checking to convert it into a structured `Graph` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e51216-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatPromptTemplate from a system and human message.\n",
    "default_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            # The system prompt contains the detailed instructions and schema for the LLM.\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Tip: Make sure to answer in the correct format and do \"\n",
    "                \"not include any explanations. \"\n",
    "                \"Use the given format to extract information from the \"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de7ca7b1-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete extraction chain using the LangChain Expression Language (LCEL) pipe operator.\n",
    "chain = default_prompt | llm | get_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baml-async-markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Processing with BAML\n",
    "\n",
    "BAML is designed to be async-first, which is highly efficient for I/O-bound tasks like making numerous calls to an LLM API. We define async helper functions to process a list of documents in parallel using Python's `asyncio` library. This is significantly faster than processing them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c61ff0-async",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the asyncio library for asynchronous programming.\n",
    "import asyncio\n",
    "from typing import List, Optional, Sequence\n",
    "\n",
    "# Define an async function to process a single response.\n",
    "async def aprocess_response(document: Document) -> GraphDocument:\n",
    "    # Asynchronously invoke our chain with the document's content.\n",
    "    resp = await chain.ainvoke({\"input\": document.page_content})\n",
    "    # Format the structured response from BAML into a LangChain GraphDocument.\n",
    "    return GraphDocument(\n",
    "        nodes=_format_nodes(resp.nodes),\n",
    "        relationships=_format_relationships(resp.relationships),\n",
    "        source=document,\n",
    "    )\n",
    "\n",
    "# Define an async function to convert a sequence of documents to GraphDocuments.\n",
    "async def aconvert_to_graph_documents(\n",
    "    documents: Sequence[Document],\n",
    ") -> List[GraphDocument]:\n",
    "    # Create a list of async tasks, one for each document.\n",
    "    tasks = [asyncio.create_task(aprocess_response(document)) for document in documents]\n",
    "    # Run all tasks concurrently and wait for them to complete.\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# Define a top-level async function to process a list of raw text strings.\n",
    "async def aprocess_text(texts: List[str]) -> List[GraphDocument]:\n",
    "    # Convert raw texts to LangChain Document objects.\n",
    "    docs = [Document(page_content=text) for text in texts]\n",
    "    # Call the conversion function to get the final graph documents.\n",
    "    graph_docs = await aconvert_to_graph_documents(docs)\n",
    "    return graph_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baml-test-markdown",
   "metadata": {},
   "source": [
    "#### Testing the BAML Chain\n",
    "Let's run a quick test on a simple sentence to see the BAML-powered chain in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb9d0143-baml-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes=[Node(id='Elon_Musk', type='Person', properties={'description': 'entrepreneur and business magnate'}), Node(id='Open_Ai', type='Organization', properties={'description': 'artificial intelligence research laboratory'})] relationships=[Relationship(source=Node(id='Elon_Musk', type='Person', properties={}), target=Node(id='Open_Ai', type='Organization', properties={}), type='PROTEST', properties={'description': None})] source=Document(metadata={}, page_content='elon musk sued open ai')\n"
     ]
    }
   ],
   "source": [
    "# Run an async process on a single test document.\n",
    "temp = await aprocess_response(Document(page_content=\"elon musk sued open ai\"))\n",
    "# Print the resulting GraphDocument.\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baml-run-markdown",
   "metadata": {},
   "source": [
    "### Running Full Extraction with BAML\n",
    "\n",
    "Now, we'll run the extraction process on a larger set of articles. We process the articles in chunks to manage memory and API call volume. The async nature of our pipeline allows for efficient processing of these chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9790d3d0-baml-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  11%|█         | 1/9 [00:15<02:02, 15.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- End of Chunk 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  22%|██▏       | 2/9 [00:30<01:43, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- End of Chunk 4 ---\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize the list for the new, successful results.\n",
    "graph_documents = []\n",
    "# Set a larger number of articles to process. (Note: reduced for demonstration purposes).\n",
    "NUM_ARTICLES = 36\n",
    "news_subset = news.head(NUM_ARTICLES)\n",
    "titles = news_subset[\"title\"]\n",
    "texts = news_subset[\"text\"]\n",
    "# Define the size of each processing chunk.\n",
    "chunk_size = 4\n",
    "\n",
    "# Loop through the articles in chunks.\n",
    "for i in tqdm(range(0, len(titles), chunk_size), desc=\"Processing Chunks\"):\n",
    "    # Get the current chunk of titles and texts.\n",
    "    title_chunk = titles[i : i + chunk_size]\n",
    "    text_chunk = texts[i : i + chunk_size]\n",
    "    # Combine title and text for each article in the chunk.\n",
    "    combined_docs = [f\"{title} {text}\" for title, text in zip(title_chunk, text_chunk)]\n",
    "\n",
    "    # Use a try-except block to handle potential errors during the async processing.\n",
    "    try:\n",
    "        # Run the async text processing function.\n",
    "        docs = await aprocess_text(combined_docs)\n",
    "        graph_documents.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n*************Error, Request failed: {str(e)}\\n\\n\")\n",
    "\n",
    "    print(f\"--- End of Chunk {i} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03c7de89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of graph documents successfully created.\n",
    "len(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baml-failure-analysis-markdown",
   "metadata": {},
   "source": [
    "### Analyzing the BAML-Enhanced Failure Rate\n",
    "\n",
    "Let's perform the same failure analysis on the results from our BAML pipeline. We expect a dramatic improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "531c518a-baml-failure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage missing: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the counter.\n",
    "empty_count = 0\n",
    "\n",
    "# Iterate through the graph documents generated by the BAML chain.\n",
    "for doc in graph_documents:\n",
    "    # Check for empty node lists.\n",
    "    if not doc.nodes:\n",
    "        empty_count += 1\n",
    "# Calculate and print the new, lower failure rate.\n",
    "print(f\"Percentage missing: {empty_count/len(graph_documents)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-conclusion",
   "metadata": {},
   "source": [
    "The results are clear: the failure rate has dropped from **75%** to **0%**. This demonstrates the power of BAML in providing the necessary structure, prompting, and error-handling to enable even small LLMs to perform complex structured data extraction tasks reliably.\n",
    "\n",
    "With a high-quality set of graph documents, we can now proceed to build and analyze our knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-progress-markdown",
   "metadata": {},
   "source": [
    "### Saving Our Progress\n",
    "\n",
    "Since the extraction process can be time-consuming, it's a good practice to save the generated `graph_documents`. We'll use `pickle` to serialize and save the list to a file, so we can easily load it back later without re-running the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pickle and os libraries for saving the results.\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure the 'data' directory exists.\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save the list of graph documents to a pickle file.\n",
    "# 'wb' mode opens the file for writing in binary format.\n",
    "with open(\"data/graph_documents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph_documents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "## Part 3: Building and Analyzing the Knowledge Graph in Neo4j\n",
    "\n",
    "In this final section, we will take our successfully extracted `GraphDocument` objects and ingest them into a Neo4j database. Once the data is in a native graph format, we can use powerful graph algorithms and queries to explore connections, identify communities, and derive meaningful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8af74df9-neo4j-connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for interacting with Neo4j.\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "# Set environment variables for Neo4j connection.\n",
    "# Replace with your actual credentials and database name.\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\" # CHANGE THIS TO YOUR PASSWORD\n",
    "os.environ[\"DATABASE\"] = \"graphragdemo\"\n",
    "\n",
    "# Initialize the Neo4jGraph instance which provides an interface to the database.\n",
    "graph = Neo4jGraph(\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    database=os.environ[\"DATABASE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neo4j-ingest-markdown",
   "metadata": {},
   "source": [
    "### Ingesting Graph Documents\n",
    "\n",
    "We use the `.add_graph_documents()` method to populate our Neo4j database. This method intelligently merges nodes with the same ID and creates relationships between them, effectively building the graph structure from our extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "509376ab-neo4j-ingest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the existing graph to start fresh.\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# Add the generated graph documents to the Neo4j graph.\n",
    "# `baseEntityLabel=True` adds a `__Entity__` label to all nodes for easier querying.\n",
    "# `include_source=True` creates a `Document` node for each source article and links entities to it.\n",
    "graph.add_graph_documents(graph_documents, baseEntityLabel=True, include_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neo4j-analysis-markdown",
   "metadata": {},
   "source": [
    "### Graph Analytics with GraphDataScience Library\n",
    "\n",
    "With our data in Neo4j, we can now use the Graph Data Science (GDS) library for advanced analysis. We'll start by exploring the basic properties of our graph.\n",
    "\n",
    "#### Entity Count vs. Token Count\n",
    "Let's see if there is a correlation between the length of an article (token count) and the number of entities extracted from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d03b7a7a-entity-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the graph to get the text and entity count for each document.\n",
    "entity_dist = graph.query(\n",
    "    \"\"\"\n",
    "MATCH (d:Document)\n",
    "RETURN d.text AS text,\n",
    "       count {(d)-[:MENTIONS]->()} AS entity_count\n",
    "\"\"\"\n",
    ")\n",
    "# Convert the query result into a pandas DataFrame.\n",
    "entity_dist_df = pd.DataFrame(entity_dist)\n",
    "# Calculate the token count for each document's text.\n",
    "entity_dist_df[\"token_count\"] = [\n",
    "    num_tokens_from_string(str(el)) for el in entity_dist_df[\"text\"]\n",
    "]\n",
    "# Create a scatter plot with a regression line to visualize the relationship.\n",
    "sns.lmplot(\n",
    "    x=\"token_count\", y=\"entity_count\", data=entity_dist_df, line_kws={\"color\": \"red\"}\n",
    ")\n",
    "plt.title(\"Entity Count vs Token Count Distribution\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Entity Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "node-degree-markdown",
   "metadata": {},
   "source": [
    "#### Node Degree Distribution\n",
    "\n",
    "Node degree is a measure of how many connections a node has. Analyzing its distribution helps us understand the overall structure of the graph. A power-law distribution (a long tail), which is common in real-world networks, would indicate the presence of a few highly connected hub nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2451f8ae-node-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy for numerical operations.\n",
    "import numpy as np\n",
    "\n",
    "# Query the graph to get the degree of each entity node.\n",
    "# The `[:!MENTIONS]` syntax excludes the MENTIONS relationship from the count.\n",
    "degree_dist = graph.query(\n",
    "    \"\"\"\n",
    "MATCH (e:__Entity__)\n",
    "RETURN count {(e)-[:!MENTIONS]-()} AS node_degree\n",
    "\"\"\"\n",
    ")\n",
    "degree_dist_df = pd.DataFrame.from_records(degree_dist)\n",
    "\n",
    "# Calculate descriptive statistics for the node degrees.\n",
    "mean_degree = np.mean(degree_dist_df[\"node_degree\"])\n",
    "percentiles = np.percentile(degree_dist_df[\"node_degree\"], [25, 50, 75, 90])\n",
    "\n",
    "# Create a histogram to visualize the distribution.\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(degree_dist_df[\"node_degree\"], bins=50, kde=False, color=\"blue\")\n",
    "# Use a logarithmic scale on the y-axis to better visualize the long tail.\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Node Degree Distribution\")\n",
    "plt.xlabel(\"Node Degree\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "\n",
    "# Add vertical lines for mean and percentiles to the plot.\n",
    "plt.axvline(\n",
    "    mean_degree,\n",
    "    color=\"red\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean: {mean_degree:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[0],\n",
    "    color=\"purple\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"25th Percentile: {percentiles[0]:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[1],\n",
    "    color=\"orange\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"50th Percentile: {percentiles[1]:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[2],\n",
    "    color=\"yellow\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"75th Percentile: {percentiles[2]:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[3],\n",
    "    color=\"brown\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"90th Percentile: {percentiles[3]:.2f}\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d120b518-descriptions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'node', 'total_count': 1879, 'non_null_descriptions': 962},\n",
       " {'type': 'relationship', 'total_count': 1450, 'non_null_descriptions': 358}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query to check the number of nodes and relationships with non-null descriptions.\n",
    "# This helps assess the richness of the extracted information.\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "MATCH (n:`__Entity__`)\n",
    "RETURN \"node\" AS type,\n",
    "       count(*) AS total_count,\n",
    "       count(n.description) AS non_null_descriptions\n",
    "UNION ALL\n",
    "MATCH (n)-[r:!MENTIONS]->()\n",
    "RETURN \"relationship\" AS type,\n",
    "       count(*) AS total_count,\n",
    "       count(r.description) AS non_null_descriptions\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-markdown",
   "metadata": {},
   "source": [
    "### Generating and Storing Node Embeddings\n",
    "\n",
    "To perform more advanced graph algorithms like similarity search and community detection, we need to represent our nodes numerically. We'll generate vector embeddings for each entity node using our `llama3` embeddings model. These embeddings capture the semantic meaning of the node's ID and description. The `Neo4jVector` library simplifies this process, automatically generating embeddings for nodes that don't have them and storing them back in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af22464f-embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding update complete.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for vector stores and the GDS library.\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "# Initialize the GDS client, pointing to the correct database.\n",
    "gds = GraphDataScience(\n",
    "    os.environ[\"NEO4J_URI\"],\n",
    "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"]),\n",
    ")\n",
    "gds.set_database(os.environ[\"DATABASE\"])\n",
    "\n",
    "# Initialize the embeddings model.\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# Initialize Neo4jVector from the existing graph.\n",
    "# It will use the 'id' and 'description' properties to generate embeddings.\n",
    "# The generated vectors will be stored in the 'embedding' property of the nodes.\n",
    "vector = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    node_label=\"__Entity__\",\n",
    "    text_node_properties=[\"id\", \"description\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    database=os.environ[\"DATABASE\"],\n",
    ")\n",
    "\n",
    "print(\"Embedding update complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knn-markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Nodes with k-Nearest Neighbors (KNN)\n",
    "\n",
    "With embeddings in place, we can find semantically similar nodes. We'll use the KNN algorithm in GDS to create `SIMILAR` relationships between nodes whose embeddings are close to each other (above a certain cosine similarity threshold). This helps enrich the graph by adding inferred connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7d3fae6-gds-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the graph into the GDS in-memory catalog.\n",
    "# This is a necessary step before running GDS algorithms.\n",
    "G, result = gds.graph.project(\n",
    "    \"entities\",                   # Name for the in-memory graph\n",
    "    \"__Entity__\",                 # Node label to project\n",
    "    \"*\",                          # Project all relationship types\n",
    "    nodeProperties=[\"embedding\"]  # Include the embedding property\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d632bfb3-knn-mutate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours: 100%|██████████| 100.0/100 [00:01<00:00, 110.46%/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ranIterations                                                            10\n",
       "nodePairsConsidered                                                  886993\n",
       "didConverge                                                            True\n",
       "preProcessingMillis                                                       2\n",
       "computeMillis                                                          1498\n",
       "mutateMillis                                                             28\n",
       "postProcessingMillis                                                      0\n",
       "nodesCompared                                                          1879\n",
       "relationshipsWritten                                                   1934\n",
       "similarityDistribution    {'min': 0.9500160217285156, 'p5': 0.9507484436...\n",
       "configuration             {'mutateProperty': 'score', 'jobId': '198c5612...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours: 100%|██████████| 100.0/100 [00:02<00:00, 110.46%/s]"
     ]
    }
   ],
   "source": [
    "# Define a similarity threshold.\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# Run the k-NN algorithm.\n",
    "# This will create new 'SIMILAR' relationships in-memory for nodes with a cosine similarity\n",
    "# score above the defined threshold.\n",
    "gds.knn.mutate(\n",
    "    G,\n",
    "    nodeProperties=[\"embedding\"],\n",
    "    mutateRelationshipType=\"SIMILAR\",\n",
    "    mutateProperty=\"score\",\n",
    "    similarityCutoff=similarity_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-resolution-markdown",
   "metadata": {},
   "source": [
    "### Entity Resolution: Merging Duplicate Nodes\n",
    "\n",
    "LLM-based extraction isn't perfect and often creates duplicate entities (e.g., \"Elon Musk\" and \"Elonmusk\"). We can combine our graph structure with another LLM call to resolve these duplicates.\n",
    "\n",
    "1.  **Candidate Identification**: We use a Cypher query to find potential duplicates. This query looks for nodes that are within the same community (identified by the Weakly Connected Components algorithm), have similar names (using Levenshtein distance via `apoc.text.distance`), and have a `SIMILAR` relationship.\n",
    "2.  **LLM-based Merging**: We then pass these candidate groups to another BAML function (`ExtractDeDupe`) which asks the LLM to decide on a canonical, single name for each group.\n",
    "3.  **Graph Refactoring**: Finally, we use the `apoc.refactor.mergeNodes` procedure in Neo4j to merge the identified duplicate nodes into a single node, cleaning up our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbcdf0ee-wcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours: 100%|██████████| 100.0/100 [00:06<00:00, 16.66%/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "writeMillis                                                             20\n",
       "nodePropertiesWritten                                                 1879\n",
       "componentCount                                                        1419\n",
       "componentDistribution    {'min': 1, 'p5': 1, 'max': 103, 'p999': 31, 'p...\n",
       "postProcessingMillis                                                     5\n",
       "preProcessingMillis                                                      0\n",
       "computeMillis                                                            5\n",
       "configuration            {'writeProperty': 'wcc', 'jobId': '4a4318a6-29...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Weakly Connected Components (WCC) algorithm based on SIMILAR relationships.\n",
    "# This assigns a community ID (`wcc`) to each node, which helps us scope our search for duplicates.\n",
    "# The `.write()` mode persists this new property back to the Neo4j database.\n",
    "gds.wcc.write(G, writeProperty=\"wcc\", relationshipTypes=[\"SIMILAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccd3e563-duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the edit distance for finding similar strings (e.g., 'Market' vs 'Markets').\n",
    "word_edit_distance = 3\n",
    "\n",
    "# This complex Cypher query identifies groups of potential duplicate entities.\n",
    "potential_duplicate_candidates = graph.query(\n",
    "    \"\"\"MATCH (e:`__Entity__`)\n",
    "    WHERE size(e.id) > 4 // longer than 4 characters\n",
    "    WITH e.wcc AS community, collect(e) AS nodes, count(*) AS count\n",
    "    WHERE count > 1\n",
    "    UNWIND nodes AS node\n",
    "    // Add text distance\n",
    "    WITH distinct\n",
    "      [n IN nodes WHERE apoc.text.distance(toLower(node.id), toLower(n.id)) < $distance | n.id] AS intermediate_results\n",
    "    WHERE size(intermediate_results) > 1\n",
    "    WITH collect(intermediate_results) AS results\n",
    "    // combine groups together if they share elements\n",
    "    UNWIND range(0, size(results)-1, 1) as index\n",
    "    WITH results, index, results[index] as result\n",
    "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "            CASE WHEN index <> index2 AND\n",
    "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                THEN apoc.coll.union(acc, results[index2])\n",
    "                ELSE acc\n",
    "            END\n",
    "    )) as combinedResult\n",
    "    WITH distinct(combinedResult) as combinedResult\n",
    "    // extra filtering\n",
    "    WITH collect(combinedResult) as allCombinedResults\n",
    "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
    "        WHERE x <> combinedResultIndex\n",
    "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "    )\n",
    "    RETURN combinedResult\n",
    "    \"\"\",\n",
    "    params={\"distance\": word_edit_distance},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3174e0ee-show-duplicates",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'combinedResult': ['David Van', 'Davidvan']},\n",
       " {'combinedResult': ['Cyb003', 'Cyb004']},\n",
       " {'combinedResult': ['Delta Air Lines', 'Delta_Air_Lines']},\n",
       " {'combinedResult': ['Elon Musk', 'Elonmusk']},\n",
       " {'combinedResult': ['Market', 'Markets']}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 groups of potential duplicates found by the query.\n",
    "potential_duplicate_candidates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29b7e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell was commented out in the original notebook.\n",
    "# It's used to reset BAML environment variables if they were set in a specific way,\n",
    "# which is not necessary for this demonstration.\n",
    "# from baml_client import reset_baml_env_vars\n",
    "# import os\n",
    "#\n",
    "# reset_baml_env_vars(dict(os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dbf378f-dedupe-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the prompts specific to the deduplication task.\n",
    "from prompts.graphragprompts import system_prompt_duplicates, user_template\n",
    "\n",
    "# Create the prompt template for this specific task.\n",
    "extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt_duplicates,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            user_template,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fde4932-chain-dedupe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the chain for entity resolution by piping the prompt, LLM, and BAML parser.\n",
    "extraction_chain = extraction_prompt | llm | get_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77be4023-dedupe-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Star Ocean: The Second Story R']\n"
     ]
    }
   ],
   "source": [
    "# Test the deduplication chain with a sample list of entities.\n",
    "entities = [\"Star Ocean The Second Story R\", \"Star Ocean: The Second Story R\"]\n",
    "print(extraction_chain.invoke(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22e13059-dedupe-function",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['David Van']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple wrapper function to call the extraction chain.\n",
    "def entity_resolution(entities: List[str]) -> Optional[List[str]]:\n",
    "    return [extraction_chain.invoke(entities)]\n",
    "\n",
    "# Test the function on the first list of candidates from our Cypher query.\n",
    "entity_resolution(potential_duplicate_candidates[0][\"combinedResult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "225f9ee0-run-dedupe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 26/26 [00:23<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for handling potential timeouts and retries.\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set the number of parallel workers.\n",
    "MAX_WORKERS = 3\n",
    "\n",
    "# Define a robust entity resolution function with retries to handle potential API failures.\n",
    "def entity_resolution(\n",
    "    entities: List[str], retries: int = 3, delay: float = 30.0\n",
    ") -> Optional[List[str]]:\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            # Invoke the chain to get the canonical entity names.\n",
    "            return [extraction_chain.invoke(entities)]\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} failed for entities: {entities}, Error: {e}\")\n",
    "            if attempt < retries:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Returning None.\")\n",
    "                return None\n",
    "\n",
    "# Process all candidate lists in parallel to get the final merged entity lists.\n",
    "merged_entities = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [\n",
    "        executor.submit(entity_resolution, el[\"combinedResult\"])\n",
    "        for el in potential_duplicate_candidates\n",
    "    ]\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "    ):\n",
    "        try:\n",
    "            to_merge = future.result()\n",
    "            if to_merge:\n",
    "                merged_entities.extend(to_merge)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in future result: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a10440ec-merge-nodes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(*)': 18}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute a Cypher query to merge the nodes in the database.\n",
    "# It iterates through the lists of duplicates and merges them into a single node using APOC procedures.\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "UNWIND $data AS candidates\n",
    "CALL {\n",
    "  WITH candidates\n",
    "  MATCH (e:__Entity__) WHERE e.id IN candidates\n",
    "  RETURN collect(e) AS nodes\n",
    "}\n",
    "CALL apoc.refactor.mergeNodes(nodes, {properties: {\n",
    "    `.*`: 'discard'\n",
    "}})\n",
    "YIELD node\n",
    "RETURN count(*)\n",
    "\"\"\",\n",
    "    params={\"data\": merged_entities},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "community-detection-markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Community Detection with Leiden Algorithm\n",
    "\n",
    "Community detection algorithms help uncover clusters of densely connected nodes in a graph. We will use the Leiden algorithm, a high-quality method that often produces better-defined communities than older algorithms like Louvain. By running it hierarchically, we can see how smaller communities merge into larger, more abstract ones at different levels of granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b22982a-gds-project-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the graph again, this time including relationship weights for the community detection algorithm.\n",
    "G, result = gds.graph.project(\n",
    "    \"communities\",  #  Name for the new in-memory graph\n",
    "    \"__Entity__\",  #  Node projection\n",
    "    {\n",
    "        \"_ALL_\": {\n",
    "            \"type\": \"*\",\n",
    "            \"orientation\": \"UNDIRECTED\",\n",
    "            \"properties\": {\"weight\": {\"property\": \"*\", \"aggregation\": \"COUNT\"}},\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f4fe617-wcc-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component count: 722\n",
      "Component distribution: {'min': 1, 'p5': 1, 'max': 305, 'p999': 305, 'p99': 14, 'p1': 1, 'p10': 1, 'p90': 4, 'p50': 1, 'p25': 1, 'p75': 2, 'p95': 6, 'mean': 2.596952908587258}\n"
     ]
    }
   ],
   "source": [
    "# Run WCC stats to see the number of disconnected components before running Leiden.\n",
    "wcc = gds.wcc.stats(G)\n",
    "print(f\"Component count: {wcc['componentCount']}\")\n",
    "print(f\"Component distribution: {wcc['componentDistribution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c67efeeb-leiden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writeMillis                                                             19\n",
       "nodePropertiesWritten                                                 1875\n",
       "ranLevels                                                                4\n",
       "didConverge                                                           True\n",
       "nodeCount                                                             1875\n",
       "communityCount                                                         732\n",
       "communityDistribution    {'min': 1, 'p5': 1, 'max': 77, 'p999': 77, 'p9...\n",
       "modularity                                                        0.970618\n",
       "modularities             [0.8718561236623066, 0.9618311533888227, 0.970...\n",
       "postProcessingMillis                                                     1\n",
       "preProcessingMillis                                                      0\n",
       "computeMillis                                                           62\n",
       "configuration            {'writeProperty': 'communities', 'theta': 0.01...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Leiden algorithm to detect communities and write the results back to the database.\n",
    "# `includeIntermediateCommunities=True` allows us to see the hierarchical structure.\n",
    "gds.leiden.write(\n",
    "    G,\n",
    "    writeProperty=\"communities\",\n",
    "    includeIntermediateCommunities=True,\n",
    "    relationshipWeightProperty=\"weight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "community-structuring-markdown",
   "metadata": {},
   "source": [
    "### Structuring Communities in the Graph\n",
    "\n",
    "The Leiden algorithm stores community IDs as a list property on each node. The following queries transform this flat list into a hierarchical graph structure by creating `(__Community__)` nodes and linking them together, as well as linking entities to their respective communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0bfaf0b-community-constraint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a uniqueness constraint on community IDs to ensure data integrity and improve query performance.\n",
    "graph.query(\n",
    "    \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:__Community__) REQUIRE c.id IS UNIQUE;\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a081a7a1-community-create",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(*)': 7500}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This query creates the community nodes and their relationships.\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "MATCH (e:`__Entity__`)\n",
    "UNWIND range(0, size(e.communities) - 1 , 1) AS index\n",
    "// Create the link from an entity to its lowest-level community\n",
    "CALL {\n",
    "  WITH e, index\n",
    "  WHERE index = 0\n",
    "  MERGE (c:`__Community__` {id: toString(index) + '-' + toString(e.communities[index])})\n",
    "  ON CREATE SET c.level = index\n",
    "  MERGE (e)-[:IN_COMMUNITY]->(c)\n",
    "  RETURN count(*) AS count_0\n",
    "}\n",
    "// Create links between hierarchical community levels\n",
    "CALL {\n",
    "  WITH e, index\n",
    "  WHERE index > 0\n",
    "  MERGE (current:`__Community__` {id: toString(index) + '-' + toString(e.communities[index])})\n",
    "  ON CREATE SET current.level = index\n",
    "  MERGE (previous:`__Community__` {id: toString(index - 1) + '-' + toString(e.communities[index - 1])})\n",
    "  ON CREATE SET previous.level = index - 1\n",
    "  MERGE (previous)-[:IN_COMMUNITY]->(current)\n",
    "  RETURN count(*) AS count_1\n",
    "}\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97c7aa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This query calculates a 'community_rank' for each community based on how many unique documents are linked to it.\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "MATCH (c:__Community__)<-[:IN_COMMUNITY*]-(:__Entity__)<-[:MENTIONS]-(d:Document)\n",
    "WITH c, count(distinct d) AS rank\n",
    "SET c.community_rank = rank;\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "community-analysis-markdown",
   "metadata": {},
   "source": [
    "### Analyzing Community Sizes\n",
    "\n",
    "Finally, let's look at the size distribution of our detected communities at each level of the hierarchy. This helps us understand the structure of the topics within our document set, from very specific small clusters to broader thematic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a08e0194-community-size",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Number of communities</th>\n",
       "      <th>25th Percentile</th>\n",
       "      <th>50th Percentile</th>\n",
       "      <th>75th Percentile</th>\n",
       "      <th>90th Percentile</th>\n",
       "      <th>99th Percentile</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.43</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.52</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.67</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.69</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Level  Number of communities  25th Percentile  50th Percentile  \\\n",
       "0     0                    858              1.0              1.0   \n",
       "1     1                    749              1.0              1.0   \n",
       "2     2                    734              1.0              1.0   \n",
       "3     3                    732              1.0              1.0   \n",
       "\n",
       "   75th Percentile  90th Percentile  99th Percentile  Max  \n",
       "0              2.0              4.0            10.43   37  \n",
       "1              2.0              5.0            18.52   77  \n",
       "2              2.0              5.0            27.67   77  \n",
       "3              2.0              5.0            27.69   77  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query to get the size (number of entities) of each community at each level.\n",
    "community_size = graph.query(\n",
    "    \"\"\"\n",
    "MATCH (c:__Community__)<-[:IN_COMMUNITY*]-(e:__Entity__)\n",
    "WITH c, count(distinct e) AS entities\n",
    "RETURN split(c.id, '-')[0] AS level, entities\n",
    "\"\"\"\n",
    ")\n",
    "community_size_df = pd.DataFrame.from_records(community_size)\n",
    "\n",
    "# Calculate percentile data for community sizes at each level.\n",
    "percentiles_data = []\n",
    "for level in sorted(community_size_df[\"level\"].unique()):\n",
    "    subset = community_size_df[community_size_df[\"level\"] == level][\"entities\"]\n",
    "    num_communities = len(subset)\n",
    "    percentiles = np.percentile(subset, [25, 50, 75, 90, 99])\n",
    "    percentiles_data.append(\n",
    "        [\n",
    "            level,\n",
    "            num_communities,\n",
    "            percentiles[0],\n",
    "            percentiles[1],\n",
    "            percentiles[2],\n",
    "            percentiles[3],\n",
    "            percentiles[4],\n",
    "            max(subset),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Create a DataFrame to display the percentile statistics for easy comparison.\n",
    "percentiles_df = pd.DataFrame(\n",
    "    percentiles_data,\n",
    "    columns=[\n",
    "        \"Level\", \"Number of communities\", \"25th Percentile\", \"50th Percentile\",\n",
    "        \"75th Percentile\", \"90th Percentile\", \"99th Percentile\", \"Max\",\n",
    "    ],\n",
    ")\n",
    "percentiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30e37a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to retrieve detailed information about the communities, including their nodes and relationships.\n",
    "# This data can be used for further summarization or visualization.\n",
    "community_info = graph.query(\n",
    "    \"\"\"\n",
    "MATCH (c:`__Community__`)<-[:IN_COMMUNITY*]-(e:__Entity__)\n",
    "WHERE c.level IN [0,1]\n",
    "WITH c, collect(e) AS nodes\n",
    "WHERE size(nodes) > 1\n",
    "CALL apoc.path.subgraphAll(nodes[0], {\n",
    "\twhitelistNodes:nodes\n",
    "})\n",
    "YIELD relationships\n",
    "RETURN c.id AS communityId,\n",
    "       [n in nodes | {id: n.id, description: n.description, type: [el in labels(n) WHERE el <> '__Entity__'][0]}] AS nodes,\n",
    "       [r in relationships | {start: startNode(r).id, type: type(r), end: endNode(r).id, description: r.description}] AS rels\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e40d932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'communityId': '0-1',\n",
       "  'nodes': [{'id': 'Chevron',\n",
       "    'description': 'Energy company',\n",
       "    'type': 'Company'},\n",
       "   {'id': 'O&G Sector', 'description': None, 'type': 'Industry'},\n",
       "   {'id': 'Q2', 'description': None, 'type': 'Period of time'}],\n",
       "  'rels': [{'start': 'Chevron',\n",
       "    'description': None,\n",
       "    'type': 'IS_A_PART_OF',\n",
       "    'end': 'O&G Sector'},\n",
       "   {'start': 'Q2',\n",
       "    'description': 'risen sharply (~25%) during that same time frame',\n",
       "    'type': 'EARNINGS_ESTIMATES',\n",
       "    'end': 'Chevron'}]}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display information for the first community as an example.\n",
    "community_info[:1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opengraphrag3",
   "language": "python",
   "name": "opengraphrag3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
